{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Set up library\n"
      ],
      "metadata": {
        "id": "W7bssXQ6JnME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nlpaug\n",
        "!pip install nltk>=3.4.5"
      ],
      "metadata": {
        "id": "g09NYmSXQORl"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from scipy.sparse import hstack\n",
        "from scipy import sparse\n",
        "\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "import nltk\n",
        "from tqdm import trange\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSOqvwC9JYnG",
        "outputId": "58dcd308-13c5-474c-b683-24a4c9103cb6"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load dataset"
      ],
      "metadata": {
        "id": "F-IWPnVuJxda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "bXTLcsN_JOyC"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"hf://datasets/tdavidson/hate_speech_offensive/data/train-00000-of-00001.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing"
      ],
      "metadata": {
        "id": "VbZ7lxzMJs-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(sample):\n",
        "    sample = sample.lower()\n",
        "    sample = re.sub(r'@\\w+', '', sample)\n",
        "    sample = re.sub('[^a-z A-Z 0-9-]+', '', sample)\n",
        "    sample = \" \".join([word for word in sample.split() if word not in stopwords.words('english') and word != 'rt'])\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "mDOB3lzbJafc"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet'] = df['tweet'].apply(lambda x : clean(x))"
      ],
      "metadata": {
        "id": "J74JAkYoPYhg"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns='class')\n",
        "y = df['class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = df['class'])"
      ],
      "metadata": {
        "id": "HIvR2HGrXR3r"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling imbalanced dataset\n",
        "- augmenting the minority classes\n",
        "- oversampling\n",
        "- SMOTE\n"
      ],
      "metadata": {
        "id": "3GSl9ezENhmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmenting"
      ],
      "metadata": {
        "id": "oXj6iP1uVP4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Replace synonyms in samples from class 0 and 2"
      ],
      "metadata": {
        "id": "OoiHiS0ic5mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augmenting_data(row):\n",
        "    aug = naw.SynonymAug(aug_src='wordnet')\n",
        "    augmented_list = aug.augment(row['tweet'])\n",
        "    row['tweet'] = augmented_list[0]  # Lấy bản đầu tiên\n",
        "    return row"
      ],
      "metadata": {
        "id": "J4pMLAkXSU5a"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.concat([X_train, y_train], axis=1)\n",
        "augmented_data = X_train[(X_train['class'] == 0) | (X_train['class'] == 2)].copy()\n",
        "augmented_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7YE9fc_UvdP",
        "outputId": "603084cf-7839-4b20-b558-583d0d678e33"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4474, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_tweet = augmented_data.apply(augmenting_data, axis=1)\n",
        "X_train = pd.concat([X_train, augmented_tweet])"
      ],
      "metadata": {
        "id": "XIlkzT5sauE8"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oversampling\n",
        "\n",
        "- Randomly selecting 1200 samples from class 0"
      ],
      "metadata": {
        "id": "D0rRGEoucwDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oversampling_data = X_train[X_train['class'] == 0].copy()\n",
        "oversampling_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEEyfd3HcmzQ",
        "outputId": "11c8ca9d-2e45-4d04-f923-2bb4c1587692"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2288, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_data = oversampling_data.sample(n=1200, random_state=42)\n",
        "X_train = pd.concat([X_train, sampled_data])"
      ],
      "metadata": {
        "id": "4VFPkqahd75l"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "3ZXPTo-QgUNz",
        "outputId": "704f591b-010e-4a9a-f2db-ee38dae35ad0"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class\n",
              "1    15352\n",
              "2     6660\n",
              "0     3488\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tf-idf vetorization"
      ],
      "metadata": {
        "id": "5asqTbQdju9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = X_train['tweet'].values\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(tweets)"
      ],
      "metadata": {
        "id": "Zki207slgsHi"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = X_train['class']\n",
        "X_train = X_train.drop(columns=['tweet', 'class'])\n",
        "\n",
        "X_train = sparse.csr_matrix(X_train)\n",
        "X_train = hstack([X_train, X_train_tfidf])"
      ],
      "metadata": {
        "id": "JjH3nQ4ajHdF"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_numeric = X_test.drop(columns=['tweet'])\n",
        "\n",
        "X_test_numeric_sparse = sparse.csr_matrix(X_test_numeric.values)\n",
        "\n",
        "X_test_tfidf = vectorizer.transform(X_test['tweet'])\n",
        "\n",
        "X_test = hstack([X_test_numeric_sparse, X_test_tfidf])\n"
      ],
      "metadata": {
        "id": "5IGvfn-josxr"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE"
      ],
      "metadata": {
        "id": "y1SbOeFNly18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_strategy = {0: 5000, 2: 8000}\n",
        "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
        "\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "ReuOXbEklZ3r"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection"
      ],
      "metadata": {
        "id": "EtaB4lpKnGi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- choose top 1000 best features"
      ],
      "metadata": {
        "id": "lWbmOJiNoOWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selector = SelectKBest(chi2, k=1000)\n",
        "X_train = selector.fit_transform(X_train, y_train)"
      ],
      "metadata": {
        "id": "7lZ78yq5nJ1S"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = selector.transform(X_test)"
      ],
      "metadata": {
        "id": "cp32A11Spy9L"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "MjwQ1XhYmtaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# 2. Dự đoán\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# 3. Đánh giá\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CduaLqYsmvcM",
        "outputId": "71cc9617-4205-40db-9e9a-413eea43ff94"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.96       286\n",
            "           1       1.00      0.99      1.00      3838\n",
            "           2       1.00      1.00      1.00       833\n",
            "\n",
            "    accuracy                           0.99      4957\n",
            "   macro avg       0.97      1.00      0.99      4957\n",
            "weighted avg       1.00      0.99      0.99      4957\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = LogisticRegression()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# 2. Dự đoán\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# 3. Đánh giá\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v67KBD82qoMe",
        "outputId": "48788ead-0284-4051-c91f-534f4be0ca91"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       286\n",
            "           1       1.00      1.00      1.00      3838\n",
            "           2       1.00      1.00      1.00       833\n",
            "\n",
            "    accuracy                           1.00      4957\n",
            "   macro avg       1.00      1.00      1.00      4957\n",
            "weighted avg       1.00      1.00      1.00      4957\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0eGNa-4qrEu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}